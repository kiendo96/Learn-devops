Article Directory

1. Mirror scan ImagePolicyWebhook

2. sysdig detect pod

3. cluster role

4. AppArmor

5. Pod Security Policy

6. Network Policy

7. Dockerfile detection and yaml file problems

8. pod security

9. Create SAs

10. Trivy detects image security

11. Create a secret

12. kube-bench

13. gVsior

14. Audit

15. Default Network Policy

16. falco detection output log format

kubernetes exam in action exam information

2 hours

15-20 topics

The appointment time is the same as CKA, and the results will be available within 32 hours

Full score of less than 100, 87 or 93 but a passing score of 67

simulated environment

4 environments and 1 console

NAT network segment 192.168.26.0

Mock exam questions

1. Mirror scan ImagePolicyWebhook

Switch cluster kubectl config use-context k8s

context

A container image scanner is set up on the cluster, but It's not yet fully integrated into the cluster's configuration When complete, the container image scanner shall scall scan for and reject the use of vulnerable images.

task:

You have to complete the entire task on the cluster's master node, where all services and files have been prepared and placed

Glven an incomplete configuration in directory /etc/kubernetes/aa and a functional container image scanner with HTTPS sendpitont http://192.168.26.60:1323/image_policy

1.enable the necessary plugins to create an image policy

2. validate the control configuration and chage it to an implicit deny

3. Edit the configuration to point the provided HTTPS endpoint correct

Finally, test if the configureion is working by trying to deploy the valnerable resource /csk/1/web1.yaml

problem solving ideas

ImagePolicyWebhook

Key words: image_policy, deny

1. Switch the cluster and view master, sshmaster

2. ls /etc/kubernetes/xxx

3. vi /etc/kubernetes/xxx/xxx.yaml change true to false

The address of https in vi /etc/kubernetes/xxx/xxx.yaml

Volume needs to be mounted in
4. Enable the ImagePolicyWebhook and - --admission-control-config-file=

5. systemctl restart kubelet

6.kubectl run pod1 --image=nginx

case:

Configure /etc/kubernetes/manifests/kube-apiserver.yaml

Add ImagePolicyWebhook related policies

Restart api-server, systemctl restart kubelet

Failed to create a pod after verifying the image

Modify /etc/kubernetes/admission/admission_config.yaml policy defaultAllow: true

Revalidate the image to create the pod

$ ls /etc/kubernetes/aa/

admission_config.yaml apiserver-client-cert.pem apiserver-client-key.pem external-cert.pem external-key.pem kubeconf

$ cd /etc/kubernetes/aa

$ cat kubeconf

apiVersion: v1

kind: Config

# clusters refers to the remote service.

clusters:

- cluster:

certificate-authority: /etc/kubernetes/aa/external-cert.pem # CA for verifying the remote service.

server: http://192.168.26.60:1323/image_policy # URL of remote service to query. Must use 'https'.

name: image-checker

contexts:

- context:

cluster: image-checker

user: api-server

name: image-checker

current-context: image-checker

preferences: {}

# users refers to the API server's webhook configuration.

users:

- name: api-server

user:

client-certificate: /etc/kubernetes/aa/apiserver-client-cert.pem # cert for the webhook admission controller to use

client-key: /etc/kubernetes/aa/apiserver-client-key.pem # key matching the cert

$ cat admission_config.yaml

apiVersion: apiserver.config.k8s.io/v1

kind: AdmissionConfiguration

plugins:

- name: ImagePolicyWebhook

configuration:

imagePolicy:

kubeConfigFile: /etc/kubernetes/aa/kubeconf

allowTTL: 50

denyTTL: 50

retryBackoff: 500

defaultAllow: false

#Modify the api-server configuration

$ cat /etc/kubernetes/manifests/kube-apiserver.yaml

...............

- command:

- kube-apiserver

- --admission-control-config-file=/etc/kubernetes/aa/admission_config.yaml # Add this line
- --advertise-address=192.168.211.40

- --allow-privileged=true

- --authorization-mode=Node,RBAC

- --client-ca-file=/etc/kubernetes/pki/ca.crt

- --enable-admission-plugins=NodeRestriction,ImagePolicyWebhook # # modify this line
- --enable-bootstrap-token-auth=true

- --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt

  ...........

- mountPath: /etc/kubernetes/pki

name: k8s-certs

readOnly: true

- mountPath: /etc/kubernetes/aa # Add this line
name: k8s-admission # Add this line
readOnly: true # Add this line
..............

- hostPath: # Add this line
path: /etc/kubernetes/aa # Add this line
type: DirectoryOrCreate # Add this line
name: k8s-admission # Add this line
- hostPath:

path: /usr/local/share/ca-certificates

type: DirectoryOrCreate

name: usr-local-share-ca-certificates

- hostPath:

path: /usr/share/ca-certificates

type: DirectoryOrCreate

name: usr-share-ca-certificates

status: {}

$ k get nodes

NAME STATUS ROLES AGE VERSION

master Ready control-plane,master 9d v1.20.1

node1 Ready <none> 9d v1.20.1

node2 Ready <none> 9d v1.20.1

#Creating pods fails

$ k run test --image=nginx

Error from server (Forbidden): pods "test" is forbidden: Post "https://external-service:1234/check-image?timeout=30s": dial tcp: lookup external-service on 8.8.8.8:53: no such host

#Modify the admission_config.yaml configuration

$ vim /etc/kubernetes/aa/admission_config.yaml

apiVersion: apiserver.config.k8s.io/v1

kind: AdmissionConfiguration

plugins:

- name: ImagePolicyWebhook

configuration:

imagePolicy:

kubeConfigFile: /etc/kubernetes/aa/kubeconf

allowTTL: 50

denyTTL: 50

retryBackoff: 500

defaultAllow: true # Modify this line to true

#Restart api-server

$ ps -ef |grep api

root 78871 39023 0 20:17 pts/3 00:00:00 grep --color=auto api

$ mv .. /kube-apiserver.yaml .

#The pod is created successfully

$ k run test --image=nginx

pod/test created

2. sysdig detect pod

Switch cluster kubectl config use-context k8s

you may user you brower to open one additonal tab to access sysdig's documentation ro Falco's documentation

Task:

user runtime detection tools to detect anomalous processes spawning and executing frequently in the sigle container belorging to pod redis.

Tow tools are available to use:

sysdig

falico

The tools are pre-installed on the cluster's worker node only; they are not available on the base system or the master node.

using the tool of your choice (including any non pre-install tool) analyze the container's behavior for at least 30 seconds, using filers that detect newly spawing and executing processes store an incident file at /opt/2/report, containing the detected incidents one per line in the follwing format:

[timestamp],[uid],[processName]

problem solving ideas

Sysdig User Guide

Key word: sysdig

0. Remember to use sysdig -l |grep to search for relevant fields
1. Switch the cluster, query the corresponding pod, and SSH to the node host corresponding to the pod

2. Using sysdig, pay attention to the required format and time, and redirect the result to the corresponding file
3. sysdig -M 30 -p "*%evt.time,%user.uid,%proc.name" container.id=container id >/opt/2/report

the case

3. cluster role

Switch cluster kubectl config use-context k8s

context

A Role bound to a pod's serviceAccount grants overly permissive permission

Complete the following tasks to reduce the set of permissions.

task

Glven an existing Pod name web-pod running in the namespace monitoring Edit the Rolebound to the Pod's serviceAccount sa-dev-1 to only allow performing list operations, only on resources of type Endpoints

create a new Role named role-2 in the namespaces monitoring which only allows performing update operations, only on resources of type persistent voume claims.

create a new Rolebind name role role-2-binding binding the newly created Roleto the Pod's serviceAccount

problem solving ideas

RBAC

Key words: role, rolebinding

1. Find rollebind, modify permissions to list and endpoints

$ kubectl edit role role-1 -n monitoring

2. Remember that --verb is permission --resource is an object
$ kubectl create role role-2 --verb=update --resource=persistentvolumeclaims -n

monitoring

3. Create a binding The binding is the corresponding sa

$ kubectl create rolebinding role-2-bindding --role=role-2 --

serviceaccount=monitoring:sa-dev-1 -n monitoring

4. AppArmor

Switch cluster kubectl config use-context k8s

Context

AppArmor is enabled on the cluster's worker node. An AppArmor profile is prepared, but not enforced yet. You may use your browser to open one additional tab to access

theAppArmor documentation. Task

On the cluster's worker node, enforce the prepared AppArmor profile located at /etc/apparmor.d/nginx_apparmor . Edit the prepared manifest file located at /cks/4/pod1.yaml to apply the AppArmor profile. Finally, apply the manifest file and create the pod specified in it

problem solving ideas

apparmor

Key words: apparmor

1. Switch the group, remember to view nodes, ssh to node nodes
2. Check the corresponding profile and name
$ cd /etc/apparmor.d

$ vi nginx_apparmor

$ apparmor_status | grep nginx-profile-3 # No grep to indicate no startup
$ apparmor_parser -q nginx_apparmor # load to enable this configuration file
3. Modify the corresponding YAML application rule, open the URL of the official website to copy the example, and modify the container name and local configuration name
$ vi /cks/4/pod1.yaml

apiVersion: v1

kind: Pod

metadata:

name: hello-apparmor

annotations:

container.apparmor.security.beta.kubernetes.io/hello: localhost/nginx-profile-3

spec:

containers:

- name: hello

image: busybox

command: [ "sh", "-c", "echo 'Hello AppArmor!' && sleep 1h" ]

4. Create it after modification
$ kubectl apply -f /cks/4/pod1.yaml

5. Pod Security Policy

Switch cluster kubectl config use-context k8s63

context

A PodsecurityPolicy shall prevent the create on of privileged Pods in a specific

namespace. Task

Create a new PodSecurityPolicy named prevent-psp-policy , which prevents the creation of privileged Pods.

Create a new ClusterRole named restrict-access-role, which uses the newly created PodSecurityPolicy prevent-psp-policy.

Create a new serviceAccount named pspdenial-sa in the existing namespace development.

Finally, create a new clusterRoleBinding named dany-access-bind , which binds the newly created ClusterRole restrict-access-role to the newly created serviceAccount

problem solving ideas

PodSecurityPolicy

Key words: PSP policy privileged

0. Switch the cluster to see if it is enabled
$ vi /etc/kubernetes/manifests/kube-apiserver.yaml

- --enable-admission-plugins=NodeRestriction,PodSecurityPolicy

$ systemctl restart kubelet

1. Copy the PSP from the official website, modify and refuse privileges
$ cat psp.yaml

apiVersion: policy/v1beta1

kind: PodSecurityPolicy

metadata:

name: prevent-psp-policy

spec:

privileged: false

seLinux:

rule: RunAsAny

supplementalGroups:

rule: RunAsAny

runAsUser:

rule: RunAsAny

fsGroup:

rule: RunAsAny

volumes:

- '*'

$ kubectl create -f psp.yaml

2. Create a corresponding clusterrole

$ kubectl create clusterrole restrict-access-role --verb=use --resource=podsecuritypolicy --resource-name=prevent-psp-policy

3. Create sa to see the corresponding ns

$ kubectl create sa psp-denial-sa -n development

4. Create a binding
$ kubectl create clusterrolebinding dany-access-bind --clusterrole=restrict-access-role --serviceaccount=development:psp-denial-sa

6. Network Policy

Switch cluster kubectl config use-context k8s

create a NetworkPolicy named pod-access to restrict access to Pod products-service running in namespace development . only allow the following Pods to connect to Pod productsservice :

Pods in the namespace testing

Pods with label environment: staging , in any namespace Make sure to apply the NetworkPolicy. You can find a skeleton on manifest file at /cks/6/p1.yaml

problem solving ideas

NetworkPolicy

NetworkPolicy
1. The host checks the pod's label
$ kubectl get pod -n development --show-labels

2. Check the label corresponding to ns, there is no need to set it
$ kubectl label ns testing name=testing

3. Orchestrate the networkpolicy policy
$ cat /cks/6/p1.yaml

kind: NetworkPolicy

metadata:

name: "pod-access"

namespace: "development"

spec:

podSelector:

matchLabels:

environment: staging

policyTypes:

- Ingress

ingress:

- from:

- namespaceSelector:

matchLabels:

name: testing

- from:

- namespaceSelector:

matchLabels:

podSelector:

matchLabels:

environment: staging

         

$ kubectl create -f /cks/6/p1.yaml

7. Dockerfile detection and yaml file problems

Switch cluster kubectl config use-context k8s

task

Analyze and edit the given Dockerfile (based on the ubuntu:16.04 image) /cks/7/Dockerfile fixing two instructions present in the file being prominent security/best-practice issues.

Analyze and edit the given manifest file /cks/7/deployment.yaml

fixing two fields present in the file being prominent security/best-practice issues.

problem solving ideas

Closing keywords: Dockerfile issues

1. Pay attention to the number of errors prompted by dockerfile

Note: USER root

2. Yaml problem: Pay attention to the API version problem, and the privileged network, mirror version, but also depends on the error mentioned in the title
case:

Dockerfile

# build container stage 1

FROM ubuntu:20.04

ARG DEBIAN_FRONTEND=noninteractive

RUN apt-get update && apt-get install -y golang-go=2:1.13~1ubuntu2

COPY app.go .

RUN pwd

RUN CGO_ENABLED=0 go build app.go

# app container stage 2

FROM alpine:3.12.0

RUN addgroup -S appgroup && adduser -S appuser -G appgroup -h /home/appuser

RUN rm -rf /bin/*

COPY --from=0 /app /home/appuser/

USER appuser

CMD ["/home/appuser/app"]

8. pod security

Switch cluster kubectl config use-context k8s

context

lt is best-practice to design containers to best teless and immutable. Task

lnspect Pods running in namespace testing and delete any Pod that is either not stateless or not immutable. use the following strict interpretation of stateless and immutable:

Pods being able to store data inside containers must be treated as not stateless.

You don't have to worry whether data is actually stored inside containers or not already. Pods being configured to be privileged in any way must be treated as potentially not stateless and not immutable.

problem solving ideas

Stateless immutable

1. Get all pods

2. Check to see if you have privileged privi*

3. Check to see if there is a volume

4. Delete both privileged networks and volumes

$ kubectl get pod pod1 -n testing -o jsonpath={.spec.volumes} | jq

$ kubectl get pod sso -n testing -o yaml |grep "privi.*: true"

$ kubectl delete pod xxxxx -n testing

 

9. Create SAs

Switch cluster kubectl config use-context k8s

 

context

 

A Pod fails to run because of an incorrectly specified ServiceAcccount.

 

task

 

create a new ServiceAccount named frontend-sa in the existing namespace qa , which must not have access to any secrets.lnspect the Pod named frontend running in the namespace qa . Edit the Pod to use the newly created serviceAccount

 

 

 

problem solving ideas

 

Configure Service Accounts for Pods

 

Keyword: ServiceAccount "must not have access to any secrets"

1. Get the sa template

$ kubectl create serviceaccount frontend-sa -n qa --dry-run -o yaml

2. Find automatic mounting through official documents

$ k edit pod frontend -n qa

apiVersion: v1

kind: Pod

metadata:

creationTimestamp: null

labels:

run: frontend

name: frontend

spec:

serviceAccountName: frontend-sa #add this line

automountServiceAccountToken: false #Add this line

containers:

- image: nginx

name: frontend

resources: {}

dnsPolicy: ClusterFirst

restartPolicy: Always

status: {}

 

3. Modify the serviceAccountName in the pod

4. Create a pod to delete other sa

10. Trivy detects image security

Switch cluster kubectl config use-context k8s

 

task

 

Use the Trivy open-source container scanner to detect images with severe vulnerabilities used by Pods in the namespace yavin . Look for images with High or Critical severity vulnerabilities, and delete the Pods that use those images. Trivy is pre-installed on the cluster's ma ster node only; it is not available on the base system or the worker nodes. You'll have to connect to the cluster's master node to use Trivy

 

 

 

problem solving ideas

 

Keywords: Trivy scanner High or Critical

1. Switch the cluster, ssh to the corresponding master

2. Get pod to scan the corresponding image, there can be no High or Critical

$ docker run ghcr.io/aquasecurity/trivy:latest image nginx:latest |grep 'High| Critical'

3. Delete the problematic image pod

$ docker rmi <image>

11. Create a secret

Switch cluster kubectl config use-context k8s

 

task

 

Retrieve the content of the existing secret named db1-test in the istio-system namespace. store the username field in a file named /cks/11/old-username.txt , and the password field in a

 

file named /cks/11/old-pass.txt. You must create both files; they don't existyet. Do not use/modify the created files in! the following steps, create new temporary files if needed. Create a new secret named test-workflow in the istio-system namespace, with the following content:

 

 

 

username: thanos

 

password : hahahaha

 

Finally, create a new Pod that has access to the secret test-workflow via volume:

 

 

 

pod name dev-pod

 

namespace istio-system

 

container name dev-container

 

image nginx:1.9

 

volume name dev-volume

 

mount path /etc/test-secret

 

problem solving ideas

 

Secret

 

Keyword: secret

1. Get the username and passwd of db1-test

$ kubectl get secrets db1-test -n istio-system -o yaml

$ echo -n "aGFoYTAwMQ==" | base64 -d > /cks/11/old-pass.txt

$ echo -n "dG9t" | base64 -d > /cks/11/old-username.txt

 

2. Create a secret named test-workflow

$ kubectl create secret generic test-workflow --from-literal=username=thanos --from-literal=password=hahahaha -n istio-system

 

3. More pods that need to create secrets

$ cat secret-pod.yaml

apiVersion: v1

kind: Pod

metadata:

name: dev-pod

namespace: istio-system

spec:

containers:

- name: dev-container

image: nginx:1.9

volumeMounts:

- name: foo

mountPath: "/etc/test-secret"

readOnly: true

volumes:

- name: dev-volume

secret:

secretName: test-workflow

 

k create -f secret-pod.yaml

12. kube-bench

Switch cluster kubectl config use-context k8s65

 

context

 

ACIS Benchmark tool was run against the kubeadm-created cluster and found multiple issues that must be addressed immediately.

 

Fix all issues via configuration and restart the affected components to ensure the

 

new settings take effect. Fix all of the following violations that were found against the API server:

 

 

 

Ensure that the

 

1.2.7 --authorization-mode FAIL argument is not set to AlwaysAllow

 

Ensure that the

 

1.2.8 --authorization-mode FAIL argument includes Node

 

Ensure that the

 

1.2.9 --authorization-mode FAIL argument includes RBAC

 

Ensure that the

 

1.2.18 --insecure-bind-address FAIL argument is not set

 

Ensure that the

 

1.2.19 --insecure-port FAIL argument is set to 0

 

Fix all of the following violations that were found against the kubelet:

 

 

 

Ensure that the

 

4.2.1 anonymous-auth FAIL argument is set to false

 

Ensure that the

 

4.2.2 --authorization-mode FAIL argument is not set to AlwaysAllow

 

Use webhook authn/authz

 

 

 

problem solving ideas

 

Keyword: Look at the entry to determine whether it is a scan

1. Switch the machine to the corresponding ssh to the master node

2. kube-bench run to find the corresponding entry, and then repair

docker run --pid=host -v /etc:/etc:ro -v /var:/var:ro -t aquasec/kube-bench:latest master --version 1.20

There is an ETCD in the exam

Case 1

 

$ docker run --pid=host -v /etc:/etc:ro -v /var:/var:ro -t aquasec/kube-bench:latest master --version 1.20

...

[FAIL] 1.1.12 Ensure that the etcd data directory ownership is set to etcd: etcd (Automated)

.........

1035234-20181020215539574-213176954.png

 

Case 2

 

1035234-20181020215539574-213176954.png

 

1035234-20181020215539574-213176954.png

 

$ cat /etc/kubernetes/kubelet.conf

apiVersion: v1

clusters:

- cluster:

certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUM1ekNDQWMrZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRJeE1Ea3hNekE0TVRjd09Wb1hEVE14TURreE1UQTRNVGN3T1Zvd0ZURVRNQkVHQTFV RQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBT1BBCnd5ZFljYlJBWjdGMmpRampHSXFWZlBTZHlVeFMxTEIwZTBKaGd0YjFyVXBtdjEydUNtZlVQaElEUnZ6dktoZnMKeXIwNmF2Tm5zZkl2UnpyK3pqMXpDT1gz VFNaYmY0a0NaOE44OEpSSUR0NnBDS0lJU0xlOHVrc3VKTzA5NWVqdgpuVnVvR21CRmVLbGN1ejFHS1FLVEw3alNaNys0TXJNYXlFOUhkbmJ6dVpNVE42ZlNvRXhGMXhxM29DMGkrZUJCCjNUK1BjMXl5V0NNcndXWEc5VUZmNFo4eFhEaGduL2hESkhKRVJ2eWtsRmpxeGpaRCt4 UHlLcTg4dk85SytKbVYKbHk2TGw1a21lbDdPdE9ZTURnMWkwUzBJUWZJMGtUaU92d0NsOHM1NVBXUThtTmtZcnlJWXhLTkJBTy94L1FCOAowMFlrNGFmVkRJQllXZHVIcjFrQ0F3RUFBYU5DTUVBd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU1CQWY4 d0hRWURWUjBPQkJZRUZHOU5hRE1R== 1V0NXaUEKNnZibGpIRnJKQnc1a0UxT3cvR05LOUhUVFI5OXp1b2U4THJSb2pzUEFUZi92ekFKZExRa2k3MXJpWXRzRkUwNApZT3JlcUE4Y2ZNeGRuUGNBeG85Z1JWQzBhQzBEd2FReC9aN StjRTcwVW15dnFQcm44VGJ6MU1uOWt1V2VQTWE5Ck9XNGI3U3RiOVp6NlBCN3pqSzk3dHhzeHRFRWV1Ky9MeDJXKwotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg== 1V0NXaUEKNnZibGpIRnJKQnc1a0UxT3cvR05LOUhUVFI5OXp1b2U4THJSb2pzUEFUZi92ekFKZExRa2k3MXJpWXRzRkUwNApZT3JlcUE4Y2ZNeGRuUGNBeG85Z1JWQzBhQzBEd2FReC9aN StjRTcwVW15dnFQcm44VGJ6MU1uOWt1V2VQTWE5Ck9XNGI3U3RiOVp6NlBCN3pqSzk3dHhzeHRFRWV1Ky9MeDJXKwotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==

server: https://192.168.211.40:6443

name: kubernetes

contexts:

- context:

cluster: kubernetes

user: system: node: master

name: system:node:master@kubernetes

current-context: system:node:master@kubernetes

kind: Config

preferences: {}

users:

- name: system:node:master

user:

client-certificate: /var/lib/kubelet/pki/kubelet-client-current.pem

client-key: /var/lib/kubelet/pki/kubelet-client-current.pem

 

 

$ echoLS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUM1ekNDQWMrZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRJeE1Ea3hNekE0TVRjd09Wb1hEVE14TURreE1UQTRNVGN3T1Zvd0ZURVRNQk VHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBT1BBCnd5ZFljYlJBWjdGMmpRampHSXFWZlBTZHlVeFMxTEIwZTBKaGd0YjFyVXBtdjEydUNtZlVQaElEUnZ6dktoZnMKeXIwNmF2Tm5zZkl2UnpyK3pqMX pDT1gzVFNaYmY0a0NaOE44OEpSSUR0NnBDS0lJU0xlOHVrc3VKTzA5NWVqdgpuVnVvR21CRmVLbGN1ejFHS1FLVEw3alNaNys0TXJNYXlFOUhkbmJ6dVpNVE42ZlNvRXhGMXhxM29DMGkrZUJCCjNUK1BjMXl5V0NNcndXWEc5VUZmNFo4eFhEaGduL2hESkhKRVJ2eWtsRmpxeG paRCt4UHlLcTg4dk85SytKbVYKbHk2TGw1a21lbDdPdE9ZTURnMWkwUzBJUWZJMGtUaU92d0NsOHM1NVBXUThtTmtZcnlJWXhLTkJBTy94L1FCOAowMFlrNGFmVkRJQllXZHVIcjFrQ0F3RUFBYU5DTUVBd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU 1CQWY4d0hRWURWUjBPQkJZRUZHOU5hRE1R== | base64 -d > /etc/kubernetes/pki/apiserver-kubelet-ca.crtcrtcrtcrtcrt

 

$ cat /etc/kubernetes/pki/apiserver-kubelet-ca.crt

-----BEGIN CERTIFICATE-----

MIIC5zCCAc+gAwIBAgIBADANBgkqhkiG9w0BAQsFADAVMRMwEQYDVQQDEwprdWJl

cm5ldGVzMB4XDTIxMDkxMzA4MTcwOVoXDTMxMDkxMTA4MTcwOVowFTETMBEGA1UE

AxMKa3ViZXJuZXRlczCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAOPA

wydYcbRAZ7F2jQjjGIqVfPSdyUxS1LB0e0Jhgtb1rUpmv12uCmfUPhIDRvzvKhfs

yr06avNnsfIvRzr+zj1zCOX3TSZbf4kCZ8N88JRIDt6pCKIISLe8uksuJO095ejv

nVuoGmBFeKlcuz1GKQKTL7jSZ7+4MrMayE9HdnbzuZMTN6fSoExF1xq3oC0i+eBB

3T+Pc1yyWCMrwWXG9UFf4Z8xXDhgn/hDJHJERvyklFjqxjZD+xPyKq88vO9K+JmV

ly6Ll5kmel7OtOYMDg1i0S0IQfI0kTiOvwCl8s55PWQ8mNkYryIYxKNBAO/x/QB8

00Yk4afVDIBYWduHr1kCAwEAAaNCMEAwDgYDVR0PAQH/BAQDAgKkMA8GA1UdEwEB

/wQFMAMBAf8wHQYDVR0OBBYEFG9NaDMQIT4ss05ZSLg/WDbl6zdiMA0GCSqGSIb3

DQEBCwUAA4IBAQCikJPGwsAwaVxqlWk2oGunnG77P4eqZbYtwZLnhoviKGSubX5r

r0skRWGcTT12PCLiEc2hLvljvT/lQ3/Muv6nbYDPSx9b3EoEF2YxHqWMc5BXJU/9

NpAXc+m/7MrZAepW1sw+ncSTdH01NbQ1BBz82kFzMYNoJ+LJcExlvSkzrMuWCWiA

6vbljHFrJBw5kE1Ow/GNK9HTTR99zuoe8LrRojsPATf/vzAJdLQki71riYtsFE04

YOreqA8cfMxdnPcAxo9gRVC0aC0DwaQx/Z5+cE70UmyvqPrn8Tbz1Mn9kuWePMa9

OW4b7Stb9Zz6PB7zjK97txsxtEEeu+/Lx2W+

-----END CERTIFICATE-----

 

 

$ vim /etc/kubernetes/manifests/kube-apiserver.yaml

.....

--kubelet-certificate-authority=/etc/kubernetes/pki/apiserver-kubelet-ca.crt

...................

 

$ kubectl get pods -n kube-system | grep kube-apiserver

$ docker run --pid=host -v /etc:/etc:ro -v /var:/var:ro -t aquasec/kube-bench:latest master --version 1.20

13. gVsior

Change cluster kubectl config use-context k8s67

 

context

 

This cluster uses containerd as CRl runtime. Containerd's default runtime handler is runc . Containerd has been prepared to support an additional runtime handler ,runsc (gVisor). Task:

 

 

 

Create a RuntimeClass named untrusted using the prepared runtime handler named runsc . Update all Pods in the namespace client to run on gvisor, unless they are

 

already running on anon-default runtime handler. You can find a skeleton manifest file at /cks/13/rc.yaml

 

 

 

problem solving ideas

 

RuntimeClass

 

Keywords: gVisor

1. Switch the cluster and create a runtimeclass with the official website document

$ vim rc.yaml

apiVersion: node.k8s.io/v1beta1

kind: RuntimeClass

metadata:

name: untrusted

handler: runsc

 

$ k -f rc.yaml create

 

2. More questions require creating a pod to use this runtime

$ k edit pod mypod -n client

apiVersion: v1

kind: Pod

metadata:

name: mypod

namespace: client

spec:

runtimeClassName: untrusted

...

 

14. Audit

Switch cluster kubectl config use-context k8s

 

task

 

Enable audit logs in the cluster. To do so, enable the log backend, and ensure that:

 

 

 

logs are stored at /var/log/kubernetes/audit-logs.txt

 

log files are retained for 5 days at maximum, a number of 10 auditlog files are retained

 

A basic policy is provided at /etc/kubernetes/logpolicy/sample-policy.yaml . it only specifies what not to log. The base policy is located on the cluster's master node. Edit and extend the basic policy to log:

 

 

 

namespace changes at RequestResponse level

 

the request body of pods changes in the namespace front-apps

 

configMap and secret changes in all namespaces at the Metadata level

 

Also, add a catch-all rule to log all other requests at the Metadata level. Don't forget to apply

 

problem solving ideas

 

audit

 

Keyword: policy

1. Switch the cluster to log in to the master, then create a directory, modify yaml, and enable auditing

$ mkdir /var/log/kubernetes/

$ mkdir /etc/kubernetes/logpolicy/

$ cat /etc/kubernetes/logpolicy/sample-policy.yaml

$ cat policy.yaml

apiVersion: audit.k8s.io/v1

kind: Policy

omitStages:

- "RequestReceived"

rules:

- level: RequestResponse

resources:

-group: ""

resources: ["namespaces"]

 

-level: Request

resources:

-group: "" # core API group

resources: ["pods"]

namespaces: ["front-apps"]

 

- level: Metadata

resources:

-group: ""

resources: ["secrets","configmaps"]

 

- level: Metadata

omitStages:

- "RequestReceived"

 

2. More official website documents to modify the corresponding strategy

$ vim /etc/kubernetes/manifests/kube-apiserver.yaml

- --audit-policy-file=/etc/kubernetes/logpolicy/sample-policy.yaml # add

- --audit-log-path=/var/log/kubernetes/audit-logs.txt # add

- --audit-log-maxage=5 # add

- --audit-log-maxbackup=10

...

- mountPath: /etc/kubernetes/logpolicy # add

name: audit # add

hostNetwork: true

priorityClassName: system-node-critical

volumes:

- hostPath: # add

path: /etc/kubernetes/logpolicy # add

type: DirectoryOrCreate # add

name: audit # add

 

 

3. Restart kubelet

$ systemctl restart kubelet

$ k get pods -n kube-system | grep api

$ cat /var/log/kubernetes/audit-logs.txt

15. Default Network Policy

Switch cluster kubectl config use-context k8s

 

context

 

A default-deny NetworkPolicy avoids to accident all y expose a Pod in a namespace that doesn't have any other NetworkPolicy defined.

 

Create a new default-deny NetworkPolicy named denynetwork in the namespace development for all traffic of type Ingress . The new NetworkPolicy must deny all lngress

 

traffic in the namespace development . Apply the newly created default-deny NetworkPolicy to all Pods running in namespace

 

development . You can find a skeleton manifest file

 

 

 

problem solving ideas

 

NetworkPolicy

 

Keywords: NetworkPolicy defined

1. Observe clearly whether to reject all or other conditions by default, and more questions require official documents to write yaml

$ cat denynetwork.yaml

apiVersion: networking.k8s.io/v1

kind: NetworkPolicy

metadata:

name: denynetwork

namespace: development

spec:

podSelector: {}

policyTypes:

-Ingress

 

$ k create -f denynetwork.yaml

16. falco detection output log format

1035234-20181020215539574-213176954.png

 

$ ssh node1

$ systemctl stop falco

$ falco

$ cd /etc/falco/

$ ls

falco_rules.local.yaml falco_rules.yaml falco.yaml k8s_audit_rules.yaml rules.available rules.d

 

$ rep -r "A shell was spawned in a container with an attached terminal" *

falco_rules.yaml: A shell was spawned in a container with an attached terminal (user=%user.name user_loginuid=%user.loginuid %container.info

 

 

#update configuration

root@node1:/etc/falco# cat falco_rules.local.yaml

-rule: Terminal shell in container

desc: A shell was used as the entrypoint/exec point into a container with an attached terminal.

condition: >

spawned_process and container

and shell_procs and proc.tty != 0

and container_entrypoint

and not user_expected_terminal_shell_in_container_conditions

output: >

%evt.time,%user.name,%container.name,%container.id

shell=%proc.name parent=%proc.pname cmdline=%proc.cmdline terminal=%proc.tty container_id=%container.id image=%container.image.repository)

priority: WARNING

tags: [container, shell, mitre_execution]

 

$ falco

Mon May 24 00:07:13 2021: Falco version 0.28.1 (driver version 5c0b863ddade7a45568c0ac97d037422c9efb750)

Mon May 24 00:07:13 2021: Falco initialized with configuration file /etc/falco/falco.yaml

Mon May 24 00:07:13 2021: Loading rules from file /etc/falco/falco_rules.yaml:

Mon May 24 00:07:13 2021: Loading rules from file /etc/falco/falco_rules.local.yaml: #Configuration takes effect

Mon May 24 00:07:13 2021: Loading rules from file /etc/falco/k8s_audit_rules.yaml:

Mon May 24 00:07:14 2021: Starting internal webserver, listening on port 8765

00:07:30.297671117: Warning Shell history had been deleted or renamed (user=root user_loginuid=-1 type=openat command=bash fd.name=/root/.bash_history name=/root/.bash_history path=<NA> oldpath =<NA> k8s_apache_apache_default_3ece2efb-fe49-4111-899f-10d38a61bab6_0 (id=84dd6fe8a9ad))

 

format change

00:07:33.763063865: Warning 00:07:33.763063865,root,k8s_apache_apache_default_3ece2efb-fe49-4111-899f-10d38a61bab6_0,84dd6fe8a9ad shell=bash parent=runc cmdline=b ash terminal=34816 container_id=84dd6fe8a9ad image=httpd)
